{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "#### Binlin Chi  \n",
    "#### Hanyuan Hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = 20, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL) solvers for sklearn enabled: https://intelpython.github.io/daal4py/sklearn.html\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from naive_bayes_classifier.load_data import str2word_bag\n",
    "from naive_bayes_classifier.configure import *\n",
    "from back_test import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accquiring Speeches From the Fed's Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using web crawler, can be found in *data_crawler.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_se, rst_lst = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rst_lst[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Five-Year Real Treasury Yield (from 2004 til Now)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def display_impactful_speech(rst_lst_=rst_lst, rates = rate_se, time_window=10, show_ratio=0.2):\n",
    "    speech_dates = pd.to_datetime([rst_lst[i][1] for i in range(len(rst_lst))])\n",
    "    speech_dates = pd.Series(speech_dates)\n",
    "    speech_dates.sort_values(inplace=True)\n",
    "    speech_dates = speech_dates.reset_index(drop=True).reindex(index=speech_dates)\n",
    "    \n",
    "    vol_after_speech = speech_dates.copy()\n",
    "    for date in speech_dates.index:\n",
    "        vol = np.std(rates[date:date+dt.timedelta(days=time_window)])\n",
    "        vol_after_speech[date] = vol\n",
    "        \n",
    "    vol_after_speech = vol_after_speech.sort_values(ascending=False)\n",
    "    dates_to_display = vol_after_speech[:int(len(vol_after_speech)*show_ratio)].index\n",
    "    plt.scatter(dates_to_display, rates[dates_to_display], color='red', marker='^')\n",
    "    \n",
    "    list_y = []\n",
    "    for date in dates_to_display:\n",
    "        list_y.append(min(rate_se)-0.2)\n",
    "    plt.scatter(dates_to_display, list_y, color='red', marker='^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "rate_se.plot();\n",
    "display_impactful_speech(show_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Reval Profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reval/preval analysis is a widely used tools in market impact analysis. It is defined as the price movement before and after a certain incident happens in the market. In this case study, we construct revals of 5-year treasury yield movement before and after Fed speeches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoore\\Documents\\workspace\\nlp\\nlp_project\\utils.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  tt = np.log(np.hstack((prev, rev)) / this)\n",
      "C:\\Users\\hoore\\Documents\\workspace\\nlp\\nlp_project\\utils.py:33: RuntimeWarning: invalid value encountered in log\n",
      "  tt = np.log(np.hstack((prev, rev)) / this)\n",
      "C:\\Users\\hoore\\Documents\\workspace\\nlp\\nlp_project\\utils.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  tt = np.log(np.hstack((prev, rev)) / this)\n",
      "C:\\Users\\hoore\\Documents\\workspace\\nlp\\nlp_project\\utils.py:33: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tt = np.log(np.hstack((prev, rev)) / this)\n"
     ]
    }
   ],
   "source": [
    "rev_lst = get_reval(rst_lst, rate_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rev_ar = np.vstack(rev_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "plt.plot(rev_ar.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the revals for most of the speeches are quite flat, while on the other hand there are several speeches that have long lasting market impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A Quick Look at the Important Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_impact_article_lst = [rst_lst[i][:2] for i in np.argsort(-rev_ar[:, 12])[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_impact_article_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the speech before the rate cut in August, is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Construction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would like to first use word bag as representation of each of the article. We also notice the titles may also be important so we should think about a method to generate features from them as well. However, intuitively, it might not be the best idea of treating the words in title and those in the passage alike.   \n",
    "Here we construct features in title and passage seperately and merge the embedded vector together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame([str2word_bag(itm[2], STOP_CHARS, STOP_WORDS, to_lower=True) for itm in rst_lst]).fillna(0)\n",
    "X = X_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_title = pd.DataFrame([str2word_bag(itm[1], STOP_CHARS, STOP_WORDS, to_lower=True) for itm in rst_lst]).fillna(0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trading Strategy Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We have tradable securities for this predictive signal. \n",
    "2. We use the rate for backtesting. Here we assume the duration of such securities meerly change in the testing period (several days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Systematic Parameter Search and Back Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand different combination of the hyperparameter, like how to label the sample, might have diverged result to the trading strategy. Here we use a systematic grid search algo to find some reasonable parameter sets by out of sample returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters we would like to test on including: learning_algorithm, training_set_length, target_lag, quantile_for_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of this algorithm can be found in *back_test_utils.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are listed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"param_results.pkl\", \"rb\") as fp:\n",
    "    param_rst_lst = pickle.load(fp)\n",
    "\n",
    "with open(\"params_str.pkl\", \"rb\") as fp:\n",
    "    params_lst = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(param_rst_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for idx, prd_ar in param_rst_lst:\n",
    "    plt.plot(np.cumsum((rev_ar[:, 11 + 1] - rev_ar[:, 10]) * prd_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Parameters Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_lst = []\n",
    "\n",
    "for idx, prd_ar in param_rst_lst:\n",
    "    rtn_lst.append((idx, np.cumsum((rev_ar[:, 11 + 1] - rev_ar[:, 10]) * prd_ar)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_lst.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[params for idx, params in enumerate(params_lst) if idx in [i[0] for i in rtn_lst[:5]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter intuitively, the titles are more noise than helpful features and the Random Forest models are uniformly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_prd = param_rst_lst[20][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_shift = 1\n",
    "\n",
    "plt.plot(np.cumsum((rev_ar[:, 9 + 1 + rtn_shift] - rev_ar[:, 9 + rtn_shift]) * y_prd));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_shift = 2\n",
    "\n",
    "plt.plot(np.cumsum((rev_ar[:, 9 + 1 + rtn_shift] - rev_ar[:, 9 + rtn_shift]) * y_prd));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_shift = 3\n",
    "\n",
    "plt.plot(np.cumsum((rev_ar[:, 9 + 1 + rtn_shift] - rev_ar[:, 9 + rtn_shift]) * y_prd));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtn_shift = 4\n",
    "\n",
    "plt.plot(np.cumsum((rev_ar[:, 9 + 1 + rtn_shift] - rev_ar[:, 9 + rtn_shift]) * y_prd));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alpha Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_decay = get_alpha_decay(rev_ar, y_prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(range(-9, 11), mean_decay, label=\"mean\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage two, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(774, 33188)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = PCA(500).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "clst_model = KMeans(n_clusters=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clst_model.fit(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clst = clst_model.predict(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(774, 500)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 检查聚类是否有意义\n",
    "#### 2. 如果有意义，在每个类别中分别建立NB模型，看单词权重，是否相同？\n",
    "#### 3. 编写算法，筛选特征（信息熵 e.g.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztype_title_lst = [rst_lst[i][:2] for i in np.where(y_clst == 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Frameworks for the Countercyclical Capital Buffer ', '20190329'),\n",
       " ('Agriculture and Community Banking', '20190328'),\n",
       " ('Global Shocks and the U.S. Economy', '20190328'),\n",
       " ('Welcoming Remarks', '20190325'),\n",
       " ('Brief Remarks', '20190311'),\n",
       " ('Monetary Policy: Normalization and the Road Ahead', '20190308'),\n",
       " ('Navigating Cautiously', '20190307'),\n",
       " ('Recent Economic Developments and Longer-Term Challenges', '20190228'),\n",
       " ('U.S. Economic Outlook and Monetary Policy', '20190228'),\n",
       " ('Is Economics for Me? Increasing the Participation of Black Women in Economics',\n",
       "  '20190223'),\n",
       " (\"The Future of the Federal Reserve's Balance Sheet\", '20190222'),\n",
       " (\"The Federal Reserve's Review of Its Monetary Policy Strategy, Tools, and Communication Practices\",\n",
       "  '20190222'),\n",
       " ('Encouraging Economic Development in High-Poverty Rural Communities',\n",
       "  '20190212'),\n",
       " ('A Conversation on Community Banking', '20190211'),\n",
       " ('Ideas of Order: Charting a Course for the Financial Stability Board',\n",
       "  '20190210'),\n",
       " ('Welcoming Remarks', '20190206'),\n",
       " (\"Inviting Participation: The Public's Role in Stress Testing's Next Chapter\",\n",
       "  '20190206'),\n",
       " ('Strengthening the Community Reinvestment Act: What Are We Learning?',\n",
       "  '20190201'),\n",
       " ('Monetary Policy Outlook for 2019', '20190110'),\n",
       " ('Insurance Supervision and International Engagement', '20190109')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztype_title_lst[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_title_lst = [rst_lst[i][:2] for i in np.where(y_clst == 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Where Do Banks Fit in the Fintech Stack?', '20170428'),\n",
       " ('Departing Thoughts', '20170404'),\n",
       " (\"America's Central Bank: The History and Structure of the Federal Reserve\",\n",
       "  '20170328'),\n",
       " ('Assessing Financial Stability over the Cycle', '20181207'),\n",
       " (\"The Federal Reserve's Framework for Monitoring Financial Stability\",\n",
       "  '20181128'),\n",
       " ('A New Chapter in Stress Testing', '20181109'),\n",
       " ('FinTech and the Search for Full Stack Financial Inclusion', '20181017'),\n",
       " ('Trends in Urban and Rural Community Banks', '20181004'),\n",
       " ('Getting It Right: Factors for Tailoring Supervision and Regulation of Large Financial Institutions',\n",
       "  '20180718'),\n",
       " (\"America's Vital Interest in Global Efforts to Promote Financial Stability\",\n",
       "  '20180627'),\n",
       " (\"Liquidity Regulation and the Size of the Fed's Balance Sheet\", '20180504'),\n",
       " ('Safeguarding Financial Resilience through the Cycle', '20180419'),\n",
       " (\"An Update on the Federal Reserve's Financial Stability Agenda\", '20180403'),\n",
       " ('The Roles of Consumer Protection and Small Business Access to Credit in Financial Inclusion',\n",
       "  '20180326'),\n",
       " ('The Federal Reserveâ\\x80\\x99s Regulatory Agenda for Foreign Banking Organizations: What Lies Ahead for Enhanced Prudential Standards and the Volcker Rule',\n",
       "  '20180305'),\n",
       " ('Refining the Stress Capital Buffer', '20190905'),\n",
       " ('Stress Testing: A Decade of Continuity and Change', '20190709'),\n",
       " ('Business Debt and Our Dynamic Financial System', '20190520'),\n",
       " ('The Financial Stability Board in 2019', '20190328'),\n",
       " ('The Community Reinvestment Act: How Can We Preserve What Works and Make it Better?',\n",
       "  '20190312')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_title_lst[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (rev_ar[:, 11] - rev_ar[:, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model_0 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model_1 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = X[np.where(y_clst == 0)[0], :]\n",
    "y_0 = np.sign(y[np.where(y_clst == 0)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X[np.where(y_clst == 1)[0], :]\n",
    "y_1 = np.sign(y[np.where(y_clst == 1)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rst0 = cls_model_0.fit(X_0, y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rst1 = cls_model_1.fit(X_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_joint_log_likelihood',\n",
       " '_partial_fit',\n",
       " '_update_mean_variance',\n",
       " 'class_count_',\n",
       " 'class_prior_',\n",
       " 'classes_',\n",
       " 'epsilon_',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'partial_fit',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'priors',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'sigma_',\n",
       " 'theta_',\n",
       " 'var_smoothing']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fit_rst0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255.,  44., 254.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_rst0.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46112116, 0.079566  , 0.45931284])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_rst0.class_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 33188)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_rst0.sigma_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['-', 'ability', 'abroad', 'absence', 'abundant', 'accelerated',\n",
       "       'access', 'accession', 'accomplishments', 'account',\n",
       "       ...\n",
       "       'previewing', 'products--taking', 'regulators--state', 'respectful',\n",
       "       'retelling', 'standards-development', 'state-based', 'usa--together',\n",
       "       'usas', 'veer'],\n",
       "      dtype='object', length=33188)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['inflation', 'policy', 'financial', 'federal', 'economic', 'market',\n",
       "       'rate', 'monetary', 'percent', 'economy', 'growth', 'reserve', 'rates',\n",
       "       'prices', 'credit', 'banks', 'markets', 'years', 'recent', 'time',\n",
       "       'interest', 'important', 'year', 'bank', 'central', 'business', 'labor',\n",
       "       'past', 'risk', 'funds', 'price', 'housing', 'capital', 'expectations',\n",
       "       'low', 'crisis', 'demand', 'data', 'fomc', 'mortgage', 'conditions',\n",
       "       'unemployment', 'real', 'community', 'small', 'system', 'employment',\n",
       "       'level', 'current', 'recovery', 'stability', 'consumer', 'public',\n",
       "       'balance', 'information', 'spending', 'states', 'global', 'future',\n",
       "       'continue', 'consumers', 'risks', 'today', 'increase', 'firms',\n",
       "       'investment', 'united', 'businesses', 'effects', 'lower', 'part',\n",
       "       'make', 'development', 'large', 'work', 'high', 'provide', 'loans',\n",
       "       'potential', 'securities', 'outlook', 'economies', 'asset', 'costs',\n",
       "       'households', 'committee', 'activity', 'higher', 'research', 'role',\n",
       "       'participants', 'pace', 'reserves', 'institutions', 'including',\n",
       "       'policies', 'remain', 'liquidity', 'number', 'support'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns[np.argsort(-fit_rst0.theta_[0])[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['financial', 'policy', 'federal', 'inflation', 'rate', 'market',\n",
       "       'economic', 'percent', 'reserve', 'economy', 'growth', 'monetary',\n",
       "       'banks', 'credit', 'rates', 'interest', 'years', 'work', 'time', 'bank',\n",
       "       'community', 'recent', 'important', 'unemployment', 'labor', 'low',\n",
       "       'employment', 'risk', 'central', 'year', 'prices', 'stability',\n",
       "       'markets', 'recovery', 'business', 'housing', 'institutions',\n",
       "       'conditions', 'today', 'workers', 'crisis', 'past', 'high', 'consumer',\n",
       "       'level', 'price', 'communities', 'mortgage', 'united', 'lower',\n",
       "       'capital', 'future', 'states', 'investment', 'development', 'firms',\n",
       "       'information', 'make', 'global', 'large', 'securities', 'productivity',\n",
       "       'expectations', 'households', 'risks', 'data', 'increase', 'funds',\n",
       "       'real', 'committee', 'job', 'cra', 'current', 'outlook', 'banking',\n",
       "       'system', 'demand', 'including', 'support', 'significant', 'recession',\n",
       "       'potential', 'people', 'fomc', 'public', 'part', 'lending', 'expected',\n",
       "       'board', 'research', 'consumers', 'continue', 'great', 'higher', 'pace',\n",
       "       'loans', 'greater', 'point', 'balance', 'view'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns[np.argsort(-fit_rst0.theta_[1])[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['inflation', 'policy', 'financial', 'federal', 'rate', 'economic',\n",
       "       'market', 'monetary', 'growth', 'economy', 'percent', 'reserve',\n",
       "       'rates', 'prices', 'banks', 'years', 'time', 'interest', 'important',\n",
       "       'markets', 'recent', 'labor', 'bank', 'central', 'credit', 'capital',\n",
       "       'price', 'year', 'expectations', 'system', 'crisis', 'low', 'risk',\n",
       "       'unemployment', 'community', 'employment', 'states', 'past', 'united',\n",
       "       'work', 'demand', 'real', 'current', 'funds', 'economies', 'data',\n",
       "       'level', 'global', 'increase', 'potential', 'business', 'conditions',\n",
       "       'fomc', 'stability', 'firms', 'lower', 'effects', 'today', 'housing',\n",
       "       'public', 'mortgage', 'part', 'productivity', 'higher', 'future',\n",
       "       'make', 'information', 'large', 'policies', 'high', 'continue',\n",
       "       'institutions', 'risks', 'costs', 'countries', 'banking', 'investment',\n",
       "       'output', 'significant', 'households', 'activity', 'including',\n",
       "       'consumer', 'increased', 'fiscal', 'research', 'loans', 'securities',\n",
       "       'number', 'development', 'committee', 'factors', 'role', 'asset',\n",
       "       'greater', 'recovery', '-', 'levels', 'policymakers', 'period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns[np.argsort(-fit_rst0.theta_[2])[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
